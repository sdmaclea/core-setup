From 4f040c1c7128a5ff1a993a9cc64a2051ea8546df Mon Sep 17 00:00:00 2001
From: Mike Danes <onemihaid@hotmail.com>
Date: Sat, 9 Dec 2017 12:08:20 +0200
Subject: [PATCH] Disable LowerCompare optimizations in minopts

---
 src/jit/lower.cpp | 88 ++++++++++++++++++++++++++++++++++++++++++-------------
 src/jit/lower.h   |  6 +++-
 2 files changed, 72 insertions(+), 22 deletions(-)

diff --git a/src/jit/lower.cpp b/src/jit/lower.cpp
index 576c5e5..e359004 100644
--- a/src/jit/lower.cpp
+++ b/src/jit/lower.cpp
@@ -2420,8 +2420,9 @@ GenTree* Lowering::LowerTailCallViaHelper(GenTreeCall* call, GenTree* callTarget
     return result;
 }
 
+#ifndef _TARGET_64BIT_
 //------------------------------------------------------------------------
-// Lowering::LowerCompare: Lowers a compare node.
+// Lowering::DecomposeLongCompare: Decomposes a TYP_LONG compare node.
 //
 // Arguments:
 //    cmp - the compare node
@@ -2430,22 +2431,14 @@ GenTree* Lowering::LowerTailCallViaHelper(GenTreeCall* call, GenTree* callTarget
 //    The next node to lower.
 //
 // Notes:
-//    - Decomposes long comparisons that feed a GT_JTRUE (32 bit specific).
-//    - Decomposes long comparisons that produce a value (X86 specific).
-//    - Ensures that we don't have a mix of int/long operands (XARCH specific).
-//    - Narrow operands to enable memory operand containment (XARCH specific).
-//    - Transform cmp(and(x, y), 0) into test(x, y) (XARCH/Arm64 specific but could
-//      be used for ARM as well if support for GT_TEST_EQ/GT_TEST_NE is added).
-//    - Transform TEST(x, LSH(1, y)) into BT(x, y) (XARCH specific)
-//    - Transform RELOP(OP, 0) into SETCC(OP) or JCC(OP) if OP can set the
-//      condition flags appropriately (XARCH/ARM64 specific but could be extended
-//      to ARM32 as well if ARM32 codegen supports GTF_SET_FLAGS).
-
-GenTree* Lowering::LowerCompare(GenTree* cmp)
+//    This is done during lowering because DecomposeLongs handles only nodes
+//    that produce TYP_LONG values. Compare nodes may consume TYP_LONG values
+//    but produce TYP_INT values.
+//
+GenTree* Lowering::DecomposeLongCompare(GenTree* cmp)
 {
-#ifndef _TARGET_64BIT_
-    if (cmp->gtGetOp1()->TypeGet() == TYP_LONG)
-    {
+    assert(cmp->gtGetOp1()->TypeGet() == TYP_LONG);
+
     GenTree* src1 = cmp->gtGetOp1();
     GenTree* src2 = cmp->gtGetOp2();
     assert(src1->OperIs(GT_LONG));
@@ -2648,12 +2641,34 @@ GenTree* Lowering::LowerCompare(GenTree* cmp)
     }
 
     return cmp->gtNext;
-    }
-#endif
+}
+#endif // !_TARGET_64BIT_
+
+//------------------------------------------------------------------------
+// Lowering::OptimizeConstCompare: Performs various "compare with const" optimizations.
+//
+// Arguments:
+//    cmp - the compare node
+//
+// Return Value:
+//    The original compare node if lowering should proceed as usual or the next node
+//    to lower if the compare node was changed in such a way that lowering is no
+//    longer needed.
+//
+// Notes:
+//    - Narrow operands to enable memory operand containment (XARCH specific).
+//    - Transform cmp(and(x, y), 0) into test(x, y) (XARCH/Arm64 specific but could
+//      be used for ARM as well if support for GT_TEST_EQ/GT_TEST_NE is added).
+//    - Transform TEST(x, LSH(1, y)) into BT(x, y) (XARCH specific)
+//    - Transform RELOP(OP, 0) into SETCC(OP) or JCC(OP) if OP can set the
+//      condition flags appropriately (XARCH/ARM64 specific but could be extended
+//      to ARM32 as well if ARM32 codegen supports GTF_SET_FLAGS).
+//
+GenTree* Lowering::OptimizeConstCompare(GenTree* cmp)
+{
+    assert(cmp->gtGetOp2()->IsIntegralConst());
 
 #if defined(_TARGET_XARCH_) || defined(_TARGET_ARM64_)
-    if (cmp->gtGetOp2()->IsIntegralConst())
-    {
     GenTree*       op1      = cmp->gtGetOp1();
     var_types      op1Type  = op1->TypeGet();
     GenTreeIntCon* op2      = cmp->gtGetOp2()->AsIntCon();
@@ -2806,7 +2821,6 @@ GenTree* Lowering::LowerCompare(GenTree* cmp)
 #endif
         }
     }
-    }
 
     if (cmp->OperIs(GT_TEST_EQ, GT_TEST_NE))
     {
@@ -2918,6 +2932,38 @@ GenTree* Lowering::LowerCompare(GenTree* cmp)
     }
 #endif // defined(_TARGET_XARCH_) || defined(_TARGET_ARM64_)
 
+    return cmp;
+}
+
+//------------------------------------------------------------------------
+// Lowering::LowerCompare: Lowers a compare node.
+//
+// Arguments:
+//    cmp - the compare node
+//
+// Return Value:
+//    The next node to lower.
+//
+GenTree* Lowering::LowerCompare(GenTree* cmp)
+{
+#ifndef _TARGET_64BIT_
+    if (cmp->gtGetOp1()->TypeGet() == TYP_LONG)
+    {
+        return DecomposeLongCompare(cmp);
+    }
+#endif
+
+    if (cmp->gtGetOp2()->IsIntegralConst() && !comp->opts.MinOpts())
+    {
+        GenTree* next = OptimizeConstCompare(cmp);
+
+        // If OptimizeConstCompare return the compare node as "next" then we need to continue lowering.
+        if (next != cmp)
+        {
+            return next;
+        }
+    }
+
 #ifdef _TARGET_XARCH_
     if (cmp->gtGetOp1()->TypeGet() == cmp->gtGetOp2()->TypeGet())
     {
diff --git a/src/jit/lower.h b/src/jit/lower.h
index 97b0ccd..0c95913 100644
--- a/src/jit/lower.h
+++ b/src/jit/lower.h
@@ -137,7 +137,11 @@ private:
     // Call Lowering
     // ------------------------------
     void LowerCall(GenTree* call);
-    GenTree* LowerCompare(GenTree* tree);
+#ifndef _TARGET_64BIT_
+    GenTree* DecomposeLongCompare(GenTree* cmp);
+#endif
+    GenTree* OptimizeConstCompare(GenTree* cmp);
+    GenTree* LowerCompare(GenTree* cmp);
     GenTree* LowerJTrue(GenTreeOp* jtrue);
     void LowerJmpMethod(GenTree* jmp);
     void LowerRet(GenTree* ret);
-- 
2.7.4

