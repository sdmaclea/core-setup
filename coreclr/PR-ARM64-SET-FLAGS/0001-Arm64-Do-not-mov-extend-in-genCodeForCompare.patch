From f952fdbe6823fefa212e2c67ef155c0579425dd6 Mon Sep 17 00:00:00 2001
From: Steve MacLean <sdmaclea.qdt@qualcommdatacenter.com>
Date: Tue, 12 Sep 2017 13:51:38 -0400
Subject: [PATCH] [Arm64] Do not mov extend in genCodeForCompare

---
 src/jit/codegenarm64.cpp | 38 +++++---------------------------------
 src/jit/lower.cpp        |  6 +++---
 2 files changed, 8 insertions(+), 36 deletions(-)

diff --git a/src/jit/codegenarm64.cpp b/src/jit/codegenarm64.cpp
index 640478b..14e1318 100644
--- a/src/jit/codegenarm64.cpp
+++ b/src/jit/codegenarm64.cpp
@@ -3430,22 +3430,23 @@ void CodeGen::genCodeForCompare(GenTreeOp* tree)
 
     GenTreePtr op1     = tree->gtOp1;
     GenTreePtr op2     = tree->gtOp2;
-    var_types  op1Type = op1->TypeGet();
-    var_types  op2Type = op2->TypeGet();
+    var_types  op1Type = genActualType(op1->TypeGet());
+    var_types  op2Type = genActualType(op2->TypeGet());
 
     assert(!op1->isUsedFromMemory());
     assert(!op2->isUsedFromMemory());
 
     genConsumeOperands(tree);
 
-    emitAttr cmpSize = EA_UNKNOWN;
+    emitAttr cmpSize = EA_ATTR(genTypeSize(op1Type));
+
+    assert(genTypeSize(op1Type) == genTypeSize(op2Type));
 
     if (varTypeIsFloating(op1Type))
     {
         assert(varTypeIsFloating(op2Type));
         assert(!op1->isContained());
         assert(op1Type == op2Type);
-        cmpSize = EA_ATTR(genTypeSize(op1Type));
 
         if (op2->IsIntegralConst(0))
         {
@@ -3463,35 +3464,6 @@ void CodeGen::genCodeForCompare(GenTreeOp* tree)
         // We don't support swapping op1 and op2 to generate cmp reg, imm
         assert(!op1->isContainedIntOrIImmed());
 
-        // TODO-ARM64-CQ: the second register argument of a CMP can be sign/zero
-        // extended as part of the instruction (using "CMP (extended register)").
-        // We should use that if possible, swapping operands
-        // (and reversing the condition) if necessary.
-        unsigned op1Size = genTypeSize(op1Type);
-        unsigned op2Size = genTypeSize(op2Type);
-
-        if ((op1Size < 4) || (op1Size < op2Size))
-        {
-            // We need to sign/zero extend op1 up to 32 or 64 bits.
-            instruction ins = ins_Move_Extend(op1Type, true);
-            inst_RV_RV(ins, op1->gtRegNum, op1->gtRegNum);
-        }
-
-        if (!op2->isContainedIntOrIImmed())
-        {
-            if ((op2Size < 4) || (op2Size < op1Size))
-            {
-                // We need to sign/zero extend op2 up to 32 or 64 bits.
-                instruction ins = ins_Move_Extend(op2Type, true);
-                inst_RV_RV(ins, op2->gtRegNum, op2->gtRegNum);
-            }
-        }
-        cmpSize = EA_4BYTE;
-        if ((op1Size == EA_8BYTE) || (op2Size == EA_8BYTE))
-        {
-            cmpSize = EA_8BYTE;
-        }
-
         instruction ins = tree->OperIs(GT_TEST_EQ, GT_TEST_NE) ? INS_tst : INS_cmp;
 
         if (op2->isContainedIntOrIImmed())
diff --git a/src/jit/lower.cpp b/src/jit/lower.cpp
index 2a126d3..db4699f 100644
--- a/src/jit/lower.cpp
+++ b/src/jit/lower.cpp
@@ -2348,7 +2348,7 @@ void Lowering::LowerCompare(GenTree* cmp)
     }
 #endif
 
-#ifdef _TARGET_AMD64_
+#ifdef _TARGET_64BIT_
     if (cmp->gtGetOp1()->TypeGet() != cmp->gtGetOp2()->TypeGet())
     {
         bool op1Is64Bit = (genTypeSize(cmp->gtGetOp1()->TypeGet()) == 8);
@@ -2369,7 +2369,7 @@ void Lowering::LowerCompare(GenTree* cmp)
 
             GenTree*  longOp       = op1Is64Bit ? cmp->gtOp.gtOp1 : cmp->gtOp.gtOp2;
             GenTree** smallerOpUse = op2Is64Bit ? &cmp->gtOp.gtOp1 : &cmp->gtOp.gtOp2;
-            var_types smallerType  = (*smallerOpUse)->TypeGet();
+            var_types smallerType  = genActualType((*smallerOpUse)->TypeGet());
 
             assert(genTypeSize(smallerType) < 8);
 
@@ -2390,7 +2390,7 @@ void Lowering::LowerCompare(GenTree* cmp)
             }
         }
     }
-#endif // _TARGET_AMD64_
+#endif // _TARGET_64BIT_
 
 #if defined(_TARGET_XARCH_) || defined(_TARGET_ARM64_)
     if (cmp->gtGetOp2()->IsIntegralConst())
-- 
2.7.4

