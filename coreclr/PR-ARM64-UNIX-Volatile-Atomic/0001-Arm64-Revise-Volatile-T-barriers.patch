From 508219f8a30d375f0855da5f4b652381ddfe730b Mon Sep 17 00:00:00 2001
From: Steve MacLean <sdmaclea.qdt@qualcommdatacenter.com>
Date: Thu, 8 Jun 2017 00:21:16 +0000
Subject: [PATCH] [Arm64] Revise Volatile<T> barriers

---
 src/gc/env/gcenv.base.h | 29 ++++++++++++++++++++++++++++-
 src/inc/volatile.h      | 29 ++++++++++++++++++++++++++++-
 2 files changed, 56 insertions(+), 2 deletions(-)

diff --git a/src/gc/env/gcenv.base.h b/src/gc/env/gcenv.base.h
index a4befca..56e67a3 100644
--- a/src/gc/env/gcenv.base.h
+++ b/src/gc/env/gcenv.base.h
@@ -353,7 +353,7 @@ typedef PTR_PTR_Object PTR_UNCHECKED_OBJECTREF;
 #if defined(__clang__)
 #if defined(_ARM_) || defined(_ARM64_)
 // This is functionally equivalent to the MemoryBarrier() macro used on ARM on Windows.
-#define VOLATILE_MEMORY_BARRIER() asm volatile ("dmb sy" : : : "memory")
+#define VOLATILE_MEMORY_BARRIER() asm volatile ("dmb ish" : : : "memory")
 #else
 //
 // For Clang, we prevent reordering by the compiler by inserting the following after a volatile
@@ -394,8 +394,22 @@ template<typename T>
 inline
 T VolatileLoad(T const * pt)
 {
+#if defined(_ARM64_) && defined(__clang__)
+    T val;
+    unsigned lockFreeAtomicSizeMask = (1 << 1) | (1 << 2) | (1 << 4) | (1 << 8);
+    if((1 << sizeof(T)) & lockFreeAtomicSizeMask)
+    {
+        __atomic_load((T volatile const *)pt, &val, __ATOMIC_ACQUIRE);
+    }
+    else
+    {
+        val = *(T volatile const *)pt;
+        asm volatile ("dmb ishld" : : : "memory");
+    }
+#else
     T val = *(T volatile const *)pt;
     VOLATILE_MEMORY_BARRIER();
+#endif
     return val;
 }
 
@@ -422,8 +436,21 @@ template<typename T>
 inline
 void VolatileStore(T* pt, T val)
 {
+#if defined(_ARM64_) && defined(__clang__)
+    unsigned lockFreeAtomicSizeMask = (1 << 1) | (1 << 2) | (1 << 4) | (1 << 8);
+    if((1 << sizeof(T)) & lockFreeAtomicSizeMask)
+    {
+        __atomic_store((T volatile *)pt, &val, __ATOMIC_RELEASE);
+    }
+    else
+    {
         VOLATILE_MEMORY_BARRIER();
         *(T volatile *)pt = val;
+    }
+#else
+    VOLATILE_MEMORY_BARRIER();
+    *(T volatile *)pt = val;
+#endif
 }
 
 extern GCSystemInfo g_SystemInfo;
diff --git a/src/inc/volatile.h b/src/inc/volatile.h
index 9531d98..44d1f0c 100644
--- a/src/inc/volatile.h
+++ b/src/inc/volatile.h
@@ -76,7 +76,7 @@
 #if defined(__GNUC__)
 #if defined(_ARM_) || defined(_ARM64_)
 // This is functionally equivalent to the MemoryBarrier() macro used on ARM on Windows.
-#define VOLATILE_MEMORY_BARRIER() asm volatile ("dmb sy" : : : "memory")
+#define VOLATILE_MEMORY_BARRIER() asm volatile ("dmb ish" : : : "memory")
 #else
 //
 // For GCC, we prevent reordering by the compiler by inserting the following after a volatile
@@ -120,8 +120,22 @@ T VolatileLoad(T const * pt)
     STATIC_CONTRACT_SUPPORTS_DAC_HOST_ONLY;
 
 #ifndef DACCESS_COMPILE
+#if defined(_ARM64_) && defined(__GNUC__)
+    T val;
+    unsigned lockFreeAtomicSizeMask = (1 << 1) | (1 << 2) | (1 << 4) | (1 << 8);
+    if((1 << sizeof(T)) & lockFreeAtomicSizeMask)
+    {
+        __atomic_load((T volatile const *)pt, &val, __ATOMIC_ACQUIRE);
+    }
+    else
+    {
+        val = *(T volatile const *)pt;
+        asm volatile ("dmb ishld" : : : "memory");
+    }
+#else
     T val = *(T volatile const *)pt;
     VOLATILE_MEMORY_BARRIER();
+#endif
 #else
     T val = *pt;
 #endif
@@ -166,8 +180,21 @@ void VolatileStore(T* pt, T val)
     STATIC_CONTRACT_SUPPORTS_DAC_HOST_ONLY;
 
 #ifndef DACCESS_COMPILE
+#if defined(_ARM64_) && defined(__GNUC__)
+    unsigned lockFreeAtomicSizeMask = (1 << 1) | (1 << 2) | (1 << 4) | (1 << 8);
+    if((1 << sizeof(T)) & lockFreeAtomicSizeMask)
+    {
+        __atomic_store((T volatile *)pt, &val, __ATOMIC_RELEASE);
+    }
+    else
+    {
         VOLATILE_MEMORY_BARRIER();
         *(T volatile *)pt = val;
+    }
+#else
+    VOLATILE_MEMORY_BARRIER();
+    *(T volatile *)pt = val;
+#endif
 #else
     *pt = val;
 #endif
-- 
2.7.4

